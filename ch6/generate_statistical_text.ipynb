{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c107b090",
   "metadata": {},
   "source": [
    "# Chapter 6 Sample Code\n",
    "This file produces the letter frequencies, ngrams, and uses them to produce sequences of letters using a statistical approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4671d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418cc363",
   "metadata": {},
   "source": [
    "### Download the Reuters dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c35c3d",
   "metadata": {},
   "source": [
    "The next few files parse the Reuters dataset files available here:\n",
    "https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html\n",
    "\n",
    "The files should be unzipped and untarred into the directory listed below with many .sgm files.\n",
    "\n",
    "The frequency data are then written to .pkl files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c837fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'ch6/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e526c8",
   "metadata": {},
   "source": [
    "### Parsing the Reuters sgm files into pkl frequency files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754ffa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractBodyTextFromFile(filename):\n",
    "    '''Extracts text from a filename, looking for <BODY> tags and returning the content between them.'''\n",
    "    inBody = False\n",
    "    textblock = []\n",
    "    with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.readline()\n",
    "        while len(text) > 0:\n",
    "            if '<BODY>' in text and '</BODY>' in text:\n",
    "                inBody = False\n",
    "                text = re.sub('^.*\\<BODY\\>', '', text)\n",
    "                text = re.sub('\\<\\/BODY\\>.*$', '', text)\n",
    "                textblock.append(text)\n",
    "            elif '<BODY>' in text and inBody is False:\n",
    "                text = re.sub('^.*\\<BODY\\>', '', text)\n",
    "                textblock.append(text)\n",
    "                inBody = True\n",
    "            elif '</BODY>' in text and inBody is True:\n",
    "                text = re.sub('\\<\\/BODY\\>.*$', '', text)\n",
    "                textblock.append(text)\n",
    "                inBody = False\n",
    "            elif inBody:\n",
    "                textblock.append(text)\n",
    "            text = f.readline()\n",
    "    return textblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bebeb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text_block):\n",
    "    '''Lowercase text removing newlines, leading and trailing space, and multiple spaces.'''\n",
    "    text = ' '.join(text_block)\n",
    "    text = re.sub('[\\s\\r\\n]+', ' ', text).lower()\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c009c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFrequencies(text, ngram_len, frequencies):\n",
    "    '''Updates and returns frequencies dictionary with ngrams of ngram_len from text.'''\n",
    "    for i in range(len(text)-ngram_len):\n",
    "        key = text[i:i+ngram_len]\n",
    "        if key in frequencies:\n",
    "            frequencies[key] += 1 \n",
    "        else:\n",
    "            frequencies[key] = 1\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e31723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def purge_nonletters(ngrams):\n",
    "    ''' Returns updated ngrams, removing those with non-letter non-space characters'''\n",
    "    to_delete = []\n",
    "    for key in ngrams:\n",
    "        if re.match('^[a-z\\s]+$', key):\n",
    "            pass\n",
    "        else:\n",
    "            to_delete.append(key)\n",
    "    for rejected in to_delete:\n",
    "        ngrams.pop(rejected, None)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a79c4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFreq(directory, ngram_len):\n",
    "    '''Primary routine to iterate through all .sigm files in directory and compute ngram frequencies.'''\n",
    "    sgm_files = [f for f in os.listdir(directory) if f.endswith('.sgm')]\n",
    "    ngram_freq = {}\n",
    "    for file in sgm_files:\n",
    "        extract_text_blocks = extractBodyTextFromFile(os.path.join(directory, file))\n",
    "        cleaned_block = preprocess(extract_text_blocks)\n",
    "        ngram_freq = computeFrequencies(cleaned_block, ngram_len, ngram_freq)\n",
    "        purge_nonletters(ngram_freq)\n",
    "    return ngram_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032e3128",
   "metadata": {},
   "source": [
    "Computes ngram frequencies for 1 to MAX_NGRAM_SIZE and write to freq#.pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b56ff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ngram size 1\n",
      "Length of freq dictionary size 1 27\n",
      "Processing ngram size 2\n",
      "Length of freq dictionary size 2 703\n",
      "Processing ngram size 3\n",
      "Length of freq dictionary size 3 9860\n",
      "Processing ngram size 4\n",
      "Length of freq dictionary size 4 53912\n",
      "Processing ngram size 5\n",
      "Length of freq dictionary size 5 172338\n",
      "Processing ngram size 6\n",
      "Length of freq dictionary size 6 420485\n",
      "Processing ngram size 7\n",
      "Length of freq dictionary size 7 823342\n"
     ]
    }
   ],
   "source": [
    "MAX_NGRAM_SIZE = 7\n",
    "for ngrami in range(1,MAX_NGRAM_SIZE+1):\n",
    "    print('Processing ngram size', ngrami)\n",
    "    freqOutput = computeFreq(directory, ngrami)\n",
    "    with open(os.path.join(directory, 'freq' + str(ngrami) + '.pkl'), 'wb') as f:\n",
    "        pickle.dump(freqOutput, f)\n",
    "    print('Length of freq dictionary size', ngrami, len(freqOutput))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b62bc584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncleaned 16372459 Cleaned 15620895\n"
     ]
    }
   ],
   "source": [
    "def checkTrainingSetSize(directory):\n",
    "    ''' Checks the size of the training set before and after removing non-letter characters. '''\n",
    "    sgm_files = [f for f in os.listdir(directory) if f.endswith('.sgm')]\n",
    "    training_set_size = 0\n",
    "    cleaned_set_size = 0\n",
    "    for file in sgm_files:\n",
    "        extract_text_blocks = extractBodyTextFromFile(os.path.join(directory, file))\n",
    "        training_set_size += len(' '.join(extract_text_blocks))\n",
    "        cleaned_block = preprocess(extract_text_blocks)\n",
    "        cleaned_set_size += len(cleaned_block)\n",
    "    print('Uncleaned', training_set_size, 'Cleaned', cleaned_set_size)\n",
    "checkTrainingSetSize(directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3906d092",
   "metadata": {},
   "source": [
    "Generate frequency dict of the letters within the text files.\n",
    "Write the file to letter_frequencies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57a66e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIndividualLetterFrequencies(directory):\n",
    "    '''Computes letter frequencies in the text files in the given directory.'''\n",
    "    sgm_files = [f for f in os.listdir(directory) if f.endswith('.sgm')]\n",
    "    training_set_size = 0\n",
    "    cleaned_set_size = 0\n",
    "    letter_frequencies = {}\n",
    "    for u in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        letter_frequencies[u] = 0\n",
    "    for file in sgm_files:\n",
    "        extract_text_blocks = extractBodyTextFromFile(os.path.join(directory, file))\n",
    "        training_set_size += len(' '.join(extract_text_blocks))\n",
    "        cleaned_block = preprocess(extract_text_blocks)\n",
    "        for u in cleaned_block:\n",
    "            if u in letter_frequencies:\n",
    "                letter_frequencies[u] += 1\n",
    "        cleaned_set_size += len(cleaned_block)\n",
    "    return letter_frequencies\n",
    "\n",
    "letter_frequencies = computeIndividualLetterFrequencies(directory)\n",
    "df = pd.DataFrame(list(letter_frequencies.items()), columns=['letter', 'frequency'])\n",
    "df = df.sort_values(by='frequency', ascending=False)\n",
    "df.to_csv('letter_frequencies.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb75b12",
   "metadata": {},
   "source": [
    "### Generate the next letters from the ngram frequency data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "878c7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_next_letter_from_current_frequency_table():\n",
    "    ''' Generates pairwise frequency table for current letter and next letter'''\n",
    "    with open(os.path.join(directory, 'freq2.pkl'), 'rb') as f:\n",
    "        freq2 = pickle.load(f)\n",
    "    set1 = set()\n",
    "    set2 = set()\n",
    "    for key in freq2:\n",
    "        set1.add(key[0])\n",
    "        set2.add(key[1])\n",
    "    lst1 = list(set1)\n",
    "    lst2 = list(set2)\n",
    "    lst1.sort()\n",
    "    lst2.sort()\n",
    "    freqTable = pd.DataFrame(index=lst2, columns=lst1)    \n",
    "    freqTable.fillna(0, inplace=True)\n",
    "    for key in freq2:\n",
    "        freqTable.at[key[1], key[0]] = freq2[key]\n",
    "    freqTable.to_csv(os.path.join(directory, 'freq2tbl.csv'))\n",
    "create_next_letter_from_current_frequency_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483292e2",
   "metadata": {},
   "source": [
    "The file written out was freq2tbl was used to create the histogram of the most frequent single entries and the pairs of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79061254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNormalizePkl1(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    totalsum = 0.0\n",
    "    for key, val in data.items():\n",
    "        totalsum += val\n",
    "    for key in data:\n",
    "        data[key] = data[key] / totalsum\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be5dcd",
   "metadata": {},
   "source": [
    "Writes text using the letter frequencies alone without ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d78398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  eelnin ungia wwuetthgsaaeuuoiolifs ttelaroi ic so u dnli ohsaadoiolid\n",
      "2 t ape ui  preirl y ldueeseihenw eioxtincplastmtcteughct ivlrwataartrbi\n",
      "3 ectee hcmasauisdeb  eaeacwinsarsdu enti nuee dba io haqid kh  aeeoa lr\n",
      "4 aelplilvprag el nl einrileadneirearoafr ndstey m iser e hnpatfut hllbi\n",
      "5 appdsmn  rstthylan   mvlccirefrrchneahbrgbmefw schnygtof mormde neehc \n",
      "6     dwo r amave orre mclnditytceo  oms e ap ilr  arleits a s n  mi  de\n",
      "7  dowsibits   e ehns  ai siutfc notoitieimfdwdefss hmyra aom e ss txc r\n",
      "8 svirdit ayde tmc hhriedtrnastdp tbrgdm ocildnaatrsts friu tlu fiebgzab\n",
      "9  ddaefnlcgtr aryugnit snin nothohmltmei onoroelcnowoindte  yiaipdshsle\n",
      "10  terlea i ii r dse icelnl mn wbeaaouiodm n eeiposnrarsr   rpseaee tage\n"
     ]
    }
   ],
   "source": [
    "data = loadNormalizePkl1(os.path.join(directory,'freq1.pkl'))\n",
    "random.seed(41)\n",
    "for iter in range(10):\n",
    "    outputstr = ''\n",
    "    for index in range(70):\n",
    "        prob = random.uniform(0.0, 1.0)\n",
    "        sum = 0\n",
    "        done = False\n",
    "        prevkey = ''\n",
    "        for key in data:\n",
    "            if not done:\n",
    "                sum += data[key]\n",
    "                if sum >= prob:\n",
    "                    done = True\n",
    "                    outputstr += key\n",
    "\n",
    "    print(iter+1, outputstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9657707",
   "metadata": {},
   "source": [
    "#### Produce text using the n-gram letter frequencies of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ba3d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNormalizePkln(filename):\n",
    "    ''' Creates normalization by first n-1 letters of ngram for the last letter in ngrams'''\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    norm = {}\n",
    "    for key in data.keys():\n",
    "        prefix = key[0:len(key)-1]\n",
    "        norm[prefix] = 0\n",
    "    for key,value in data.items():\n",
    "        prefix = key[0:len(key)-1]\n",
    "        norm[prefix] += value\n",
    "    for key,value in data.items():\n",
    "        prefix = key[0:len(key)-1]\n",
    "        data[key] = data[key] / norm[prefix]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94d37b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prob_string(prob, data, ngram):\n",
    "    ''' Selects random entry using prob from the data keys starting with ngram that have been normalized.'''\n",
    "    sum = 0.0\n",
    "    for key in data:\n",
    "        if key.startswith(ngram):\n",
    "            sum += data[key]\n",
    "            if sum >= prob:\n",
    "                return key[-1]  \n",
    "    print('No match found for', ngram, prob)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3310ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_random_key(data):\n",
    "    ''' Returns a random key from the data dictionary.'''\n",
    "    key = random.choice(list(data.keys()))  \n",
    "    return key\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3f83530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNgramStr(directory, ngram):\n",
    "    '''Generates lines of random text based on n-gram frequencies stored in directory.'''\n",
    "    NUM_LINES = 5\n",
    "    LETTERS_PER_LINE = 70\n",
    "    data = loadNormalizePkln(os.path.join(directory, 'freq' + str(ngram) + '.pkl'))\n",
    "    random.seed(422)\n",
    "    for iter in range(NUM_LINES):\n",
    "        outputstr = return_random_key(data)\n",
    "        for index in range(LETTERS_PER_LINE):\n",
    "            prob = random.uniform(0.0, 1.0)\n",
    "            nextchar = find_prob_string(prob, data, outputstr[-ngram+1:])\n",
    "            outputstr += nextchar\n",
    "        print(iter+1, outputstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56b2c2",
   "metadata": {},
   "source": [
    "##### Main call to generate ngram-based letter sequences using problabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7f759f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 frn in and corp told keeping an from years to market of about there listerd\n",
      "2 ow fluctane cited under fight pct on the large share into mazda ways to the\n",
      "3 orb states all year oper shr longer board if of worldwident effort said und\n",
      "4 logan outstandar years for that they said the fund dives for the right pres\n",
      "5 ority and its proving exposures said internative to tights income for throu\n"
     ]
    }
   ],
   "source": [
    "ngram_len = 5  # use ngram_len of 2 to MAX_NGRAM_SIZE = 7\n",
    "createNgramStr(directory, ngram_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical_trials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
